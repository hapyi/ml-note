# EM算法（Expection Maximization Algorithm）学习
----

### 一、背景

假设3枚硬币，分别为A、B和C。这些硬币出现的概率分别为$n$，$p$和$q$。投币实验如下，先投A，如果A是正面，即A=1，那么选择B继续投；A=0，那么选C来投。最后，如果B或者C是正面，那么$ y=1 $；是反面，那么$ y=0 $；独立重复$n$次实验（$ n = 10 $），观测结果如下：1、1、0、1、0、0、1、0、1、1。假设只能观测到投掷硬币的结果，但是无法观测到投掷硬币的过程。试问如何估计这三个硬币正面出现的概率，即$n$，$p$和$q$的值。

解：设随机变量$y$为观测值，则投掷一次的概率模型可以写成：
$$ \begin{equation} 
      P(Y|\theta ) = \quad \pi { p }^{ y}{ (1-p) }^{ 1-y } + (1 - \pi ){ q }^{ y}{ (1-q ) }^{ 1-y } 
\end{equation} $$

对于这$n$次观测，观测数据$Y$的似然函数可写为：
$$ \begin{equation} 
P(Y|\theta )=\quad \prod _{ j=1 }^{ n }{ \left[ \pi { p }^{ { y }_{ j } }{ (1-p) }^{ 1-{ y }_{ i } }+(1-\pi ){ q }^{ { y }_{ j } }{ (1-q) }^{ 1-{ y }_{ j } } \right]  } 
\end{equation} $$

对于这类问题，从观测样本推测其概率分布情况，自然而然，要使用最大似然估计的思想。
$$ \begin{align} 
\hat { \theta  } &=argmax \, { x }_{ \theta  }\log { P(Y|\theta ) } \\ 
&=argmax \,  { x }_{ \theta  }\sum _{ j=1 }^{ n }{ \log { P({ y }^{ j }|\theta ) }  } \\ 
&=argmax \,  { x }_{ \theta  }\sum _{ j=1 }^{ n }{ \log { \left[ \pi { p }^{ { y }_{ j } }{ (1-p) }^{ 1-{ y }_{ i } }+(1-\pi ){ q }^{ { y }_{ j } }{ (1-q) }^{ 1-{ y }_{ j } } \right]  }  } 
\end{align} $$

按照最大似然估计的求解步骤，对等式（5）求偏导或者使用梯度下降法，求解最大似然估计值，会带来很多问题。这是因为，对等式（5）非常复杂，求偏导非常困难。于是，我们通过引入隐变量函数$ { Q }_{ i }({ z }^{ i }) $来化简求解过程。而这就是EM算法的思想。

### 二、基础数学

嘎嘎嘎

### 三、EM算法的数学推导

当我们把这个隐变量函数$ { Q }_{ i }({ z }^{ i }) $（其中 $ \sum _{ i }{ Q }_{ i }({ z }_{ i }) = 1，{ Q }_{ i }(z) \, \ge \, 0 $））引入到似然函数$ \ell ({ \theta  }) $，整个求解过程就变为：

$$ \begin{align}
\ell (\theta ) &=\sum _{ i=1 }^{ m }{ \log   } p({ { x }^{ (i) } };\theta ) \\ 
&=\sum _{ i=1 }^{ m }{ \log   } \sum _{ { z }_{ i } }^{  }{ p({ { x }^{ (i) } },{ { z }^{ (i) } };\theta ) } \\ 
&=\sum _{ i=1 }^{ m }{ \log   } \sum _{ { { z }^{ (i) } } }^{  }{ { Q }_{ i }({ { z }^{ (i) } })\frac { p({ { x }^{ (i) } },{ { z }^{ (i) } };\theta ) }{ { Q }_{ i }({ { z }^{ (i) } }) }  } \\ 
&\ge \sum _{ i=1 }^{ m } \sum _{ { { z }^{ (i) } } }^{  }{ { Q }_{ i }({ { z }^{ (i) } })\frac { p({ { x }^{ (i) } },{ { z }^{ (i) } };\theta ) }{ { Q }_{ i }({ { z }^{ (i) } }) }  } 
\end{align} $$

等式（6）至（7）由边缘概率公式展开；等式（7）到（8）引入隐变量$ { Q }_{ i }({ z }^{ i }) $，分子分母同乘以隐变量$ { Q }_{ i }({ z }^{ i }) $；等式（8）到（9）由Jensen不等式推出。这里需要注意的是：$ \sum _{ { { z }^{ (i) } } }^{  }{ { Q }_{ i }({ { z }^{ (i) } })\left[ { p({ { x }^{ (i) } },{ { z }^{ (i) } };\theta ) }/{ { Q }_{ i }({ { z }^{ (i) } }) } \right]  }  $就是$ \left[ { p({ { x }^{ (i) } },{ { z }^{ (i) } };\theta ) }/{ { Q }_{ i }({ { z }^{ (i) } }) } \right ]  $的期望值。

根据Jensen不等可知，要使等式（9）两边取等号，则要满足：
$$ \begin{align} 
   \frac { p({ { x }^{ (i) } },{ { z }^{ (i) } };\theta ) }{ { Q }_{ i }({ { z }^{ (i) } }) }  = c 
\end{align} $$

因此，从等式（10）中看出，当取定值时，引入的隐变量分布函数$ { Q }_{ i }({ z }^{ i }) $需满足$ { Q }_{ i }({ z }^{ i }) \propto P({ { x }^{ (i) } },{ z }^{ (i) };{ \theta  }^{ (t) })  $的性质。于是：

$$ \begin{align} 
{ Q }_{ (i) }({ z }^{ (i) }) &=\frac { p({ x }^{ (i) },{ z }^{ (i) }; \theta ) }{ \sum _{ z } { P({ x }^{ (i) },{ z }^{ (i) }; \theta ) }  } \\ 
&= \frac { P({ x }^{ (i) },{ z }^{ (i) }; \theta ) }{ P({ x }^{ (i) }; \theta ) } \\ 
&= P({ z }^{ (i) }|{ x }^{ (i) }; \theta )
\end{align} $$

$ { Q }_{ i }({ z }^{ ( i )}) $是未知的，是需要计算的，否则整个计算无法继续。由于$ { Q }_{ i }({ z }^{ ( i )}) $与$ p({ { x }^{ (i) } },{ z }^{ (i) };{ \theta  }) $成正比，所以由等式（11）左边是$z$，右边是分子$x$和$z$除掉$x$就是$z$。而等式（11）到（12）是边缘概率求和，把$z$给消除了。等式（12）到（13）是贝叶斯公式。通过这几步运算，把隐变量的分布求解出来，使其从未知变成已知。因此，从（11）到（13）对隐变量进行构建，以及如何选择隐变量。

但是，等式（5）多项式中加号的存在，导致在求偏导的时候容易把其消去。而且这是个三硬币模型，我们无法得知结果是硬币B还是硬币C抛出这个隐藏函数。因此，我们把隐变量函数$ { Q }_{ j }({ z }_{ i }) $引入到似然函数中，可得：

对于等式（6）是将$ { z }_{ i } $作为隐变量，其中$ { z }_{ 1 } $为结果由硬币B抛出，$ { z }_{ 2} $为结果由硬币C抛出，不难发现：

$$ \begin{align} 
\sum _{ i=1 }^{ 2 }{ P({ y }_{ i },{ z }_{ i }|\theta ) } &= P({ y }_{ i }|\theta ) \\ 
&= \pi { p }^{ { y }_{ j } }{ (1-p) }^{ 1-{ y }_{ i } }+(1-\pi ){ q }^{ { y }_{ j } }{ (1-q) }^{ 1-{ y }_{ j } }  
\end{align} $$

从等式（7）到（8）的过程，$ { Q }_{ j }({ z }_{ i }) $表示的是关于z的某种分布（$\sum _{ i }^{ { Q }_{ j }({ z }_{ i }) } =1，{ Q }_{ i }(z) \, \ge \, 0 $），在P的分子分母同乘以$ { Q }_{ j }({ z }_{ i }) $。等式（8）到（9）使用了Jensen不等式，这里需要注意的是此外，$log$函数是凸函数。要使等式成立，则要使等式$ \frac { P({ y }_{ i },{ z }_{ i }|\theta ) }{ { Q }_{ j }({ z }_{ i }) } =c $成立。因此，



这里我们可以发现：
$$ \begin{align} 
{ Q }_{ j }({ z }_{ 1 }|\theta ) &=\quad \frac { \pi { p }^{ { y }_{ j } }{ (1-p) }^{ 1-{ y }_{ i } } }{ \pi { p }^{ { y }_{ j } }{ (1-p) }^{ 1-{ y }_{ i } }+(1-\pi ){ q }^{ { y }_{ j } }{ (1-q) }^{ 1-{ y }_{ j } } } \\ 
{ Q }_{ j }({ z }_{ 2 }|\theta ) &=\quad \frac { (1-\pi ){ q }^{ { y }_{ j } }{ (1-q) }^{ 1-{ y }_{ j } } }{ \pi { p }^{ { y }_{ j } }{ (1-p) }^{ 1-{ y }_{ i } }+(1-\pi ){ q }^{ { y }_{ j } }{ (1-q) }^{ 1-{ y }_{ j } } } 
\end{align} $$

对于等式（8），我们成功把多项式中的加号给消除了，如果知道$ { Q }_{ j }({ z }_{ i }) $的所有值，那么可以很容易地进行最大似然估计，但是$ Q $也包含$ \theta $的值。这样的话，我们可以事先给定$ {\theta}_{0} $作为初值，然后通过多次迭代，得到$ {Q}_{1} $的最大值，然后，

所有值EM算法会利用到最大似然估计的思想，但是，其是一种简便计算的方法。由已知的观测变量推测出其概率分布情况的常使用的是最大似然估计的思想，但是由于的情况。$ { a }/{ b } $

### 四、EM算法推导

选取初始值θ0初始化θ，t=0

Repeat {

      E步：

            

      M步：

            

}直到收敛


#### EM算法收敛性证明
$$ \begin{align} 
\ell ({ \theta  }^{ (t+1) }) &\ge \sum _{ i } \sum _{ { z }^{ (i) } }{ { { Q }_{ i } }^{ (t) }({ z }^{ (i) })\log  \frac { P({ { x }^{ (i) } },{ z }^{ (i) };{ \theta  }^{ (t+1) }) }{ { { Q }_{ i } }^{ (t) }({ z }^{ (i) }) }  } \\ 
&\ge \sum _{ i } \sum _{ { z }^{ (i) } }{ { { Q }_{ i } }^{ (t) }({ z }^{ (i) })\log  \frac { P({ { x }^{ (i) } },{ z }^{ (i) };{ \theta  }^{ (t) }) }{ { { Q }_{ i } }^{ (t) }({ z }^{ (i) }) }  } \\ 
&= \ell ({ \theta  }^{ (t) }) 
 \end{align} $$

整个EM可以划分为E步和M步，E步是猜，M步是反思。

### 五、小结
我们的目标是使用去拟合模型，使用最大似然的方法估计参数$ \theta $是非常困难。这里$ { z }^{ (i) } $是潜在的随机变量，如果引入，将使最大似然估计变得简单。EM算法在本质上与极大似然估计的思想相近似，只不过其在优化上引入隐变量来进行优化而已。


$$
f\left( { E }_{ { z }^{ (i) }\~ { Q }_{ i } }\left[ \frac { P({ { x }^{ (i) } },{ z }^{ (i) };{ \theta  }^{ (t+1) }) }{ { { Q }_{ i } }^{ (t) }({ z }^{ (i) }) }  \right]  \right) \ge { E }_{ { z }^{ (i) }\~ { Q }_{ i } }\left[ f\left( \frac { P({ { x }^{ (i) } },{ z }^{ (i) };{ \theta  }^{ (t+1) }) }{ { { Q }_{ i } }^{ (t) }({ z }^{ (i) }) }  \right)  \right] 
 $$


在EM算法的推导过程中，最精妙的一点就是（10）式中的分子分母同乘以隐变量的一个分布，而套上了Jensen不等式，是EM算法顺利的形成。

### 六、参考文献
1、http://cs229.stanford.edu/notes/cs229-notes8.pdf Andrew Ng关于EM算法的笔记
2、https://www.cnblogs.com/mindpuzzle/archive/2013/04/05/2998746.html EM算法学习笔记


